name: Deploy MkDocs to GitHub Pages
on:
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-python@v5
        with:
          python-version: '3.x'
      - run: python -m pip install --upgrade pip
      - run: |
          pip install mkdocs "mkdocs-material[imaging]" \
            mkdocs-roamlinks-plugin mkdocs-glightbox mkdocs-minify-plugin \
            mkdocs-rss-plugin mkdocs-redirects mkdocs-awesome-pages-plugin \
            mkdocs-git-revision-date-localized-plugin mkdocs-sitemap-plugin
      - name: Fix Obsidian image embeds -> Markdown
        run: |
            python - <<'PY'
            import re
            from pathlib import Path

            docs = Path('docs')
            imgs = {}
            for p in docs.rglob('*'):
                if p.is_file() and p.suffix.lower() in ('.png','.jpg','.jpeg','.gif','.webp','.svg'):
                    imgs[p.name.lower()] = p.relative_to(docs).as_posix()

            for md in docs.rglob('*.md'):
                text = md.read_text(encoding='utf-8')
                def repl(m):
                    raw = m.group(1)
                    fname = raw.split('|')[0].strip()
                    path = imgs.get(fname.lower())
                    if path:
                        return f'![](/{path})'
                    return f'![{fname}]({fname})'
                new = re.sub(r'!\[\[([^\]]+?\.(?:png|jpe?g|gif|webp|svg)(?:\|[^\]]*)?)\]\]', repl, text, flags=re.I)
                if new != text:
                    md.write_text(new, encoding='utf-8')
            PY
      - name: SEO + titles normalize (front-matter + H1)
        run: |
          python - <<'PY'
          import re
          from pathlib import Path
          def parse_fm(lines):
              i=0; s=e=-1
              if i<len(lines) and lines[i].strip()=='---':
                  s=i; i+=1
                  while i<len(lines) and lines[i].strip()!='---': i+=1
                  if i<len(lines) and lines[i].strip()=='---':
                      e=i; i+=1
              return s,e,i
          def upsert_fm(lines,key,val,s,e,c):
              line=f'{key}: {val}'
              if s!=-1:
                  t=None
                  for k in range(s+1,e):
                      if re.match(rf'^\s*{key}\s*:', lines[k], flags=re.I): t=k; break
                  if t is not None: lines[t]=line
                  else: lines.insert(s+1,line); e+=1; c+=1
              else:
                  lines=['---',line,'---','']+lines; s,e,c=0,2,4
              return lines,s,e,c
          def strip_md(t):
              t=re.sub(r'```.*?```','',t,flags=re.S)
              t=re.sub(r'`[^`]*`','',t)
              t=re.sub(r'!\[[^\]]*\]\([^)]+\)','',t)
              t=re.sub(r'\[([^\]]+)\]\([^)]+\)',r'\1',t)
              t=re.sub(r'<[^>]+>','',t)
              t=re.sub(r'\s+',' ',t).strip()
              return t
          root=Path('docs')
          for p in root.rglob('*.md'):
              sp=str(p).replace('\\','/')
              if '/assets/' in sp: continue
              stem=p.stem
              lines=p.read_text(encoding='utf-8',errors='ignore').splitlines()
              if lines and lines[0].startswith('\ufeff'): lines[0]=lines[0].lstrip('\ufeff')
              s,e,c=parse_fm(lines)
              body='\n'.join(lines[c:])
              body_no_h1=re.sub(r'^\s*#.*$','',body,flags=re.M)
              paras=[x.strip() for x in re.split(r'\n\s*\n',body_no_h1) if x.strip()]
              desc=''
              for para in paras:
                  if para.startswith('!'): continue
                  cand=strip_md(para)
                  if len(cand)>=40: desc=cand; break
              if not desc and paras: desc=strip_md(paras[0])
              if len(desc)>160:
                  cut=desc[:160]; desc=cut[:cut.rfind(' ')] if ' ' in cut else cut
              sample='\n'.join(lines[c:c+6])
              tags=list(dict.fromkeys(re.findall(r'(?<!\w)#([a-zA-Z0-9_\-]+)',sample)))
              lines,s,e,c=upsert_fm(lines,'title',f'"{stem}"',s,e,c)
              if desc: lines,s,e,c=upsert_fm(lines,'description',f'"{desc}"',s,e,c)
              if tags: lines,s,e,c=upsert_fm(lines,'tags','['+', '.join(tags)+']',s,e,c)
              in_code=False
              fence=re.compile(r'^\s*(`{3,}|~{3,})'); atx=re.compile(r'^\s{0,3}#{1,6}\s+.*$')
              set1=re.compile(r'^\s*={3,}\s*$'); set2=re.compile(r'^\s*-{3,}\s*$')
              j=c; done=False
              while j<len(lines):
                  if fence.match(lines[j]): in_code=not in_code; j+=1; continue
                  if not in_code:
                      if atx.match(lines[j]): lines[j]='# '+stem; done=True; break
                      if j+1<len(lines) and (set1.match(lines[j+1]) or set2.match(lines[j+1])):
                          lines[j]='# '+stem; del lines[j+1]; done=True; break
                  j+=1
              if not done: lines.insert(c,'# '+stem)
              p.write_text('\n'.join(lines)+'\n',encoding='utf-8')
          PY
      - run: mkdocs build --site-dir site
      - name: Generate sitemap.xml
        run: |
          python - <<'PY'
          import os, time, xml.etree.ElementTree as ET
          base = "https://lautarovculic.github.io"
          urls=[]
          for root,_,files in os.walk("site"):
              for f in files:
                  if not f.endswith(".html"): continue
                  path=os.path.join(root,f)
                  rel=path[len("site/"):]
                  if rel in ("404.html",): continue
                  if rel.startswith(("assets/","search/")): continue
                  url=rel[:-10] if rel.endswith("index.html") else rel
                  if not url.startswith("/"): url="/"+url
                  if url.endswith(".html"): url=url
                  loc=base+url
                  mtime=time.strftime("%Y-%m-%d", time.gmtime(os.path.getmtime(path)))
                  urls.append((loc,mtime))
          urlset=ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
          for loc,mtime in sorted(urls):
              u=ET.SubElement(urlset,"url")
              ET.SubElement(u,"loc").text=loc
              ET.SubElement(u,"lastmod").text=mtime
          ET.ElementTree(urlset).write("site/sitemap.xml", encoding="utf-8", xml_declaration=True)
          PY
      - uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
